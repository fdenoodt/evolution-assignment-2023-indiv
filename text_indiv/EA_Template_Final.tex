\documentclass[a4paper,10pt]{article}
\usepackage[utf8]{inputenc}

\usepackage[english]{babel}
\usepackage[dvinames]{xcolor}
\usepackage[compact,small]{titlesec}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{amsfonts,amsmath,amssymb}
\usepackage{marginnote}
\usepackage[top=1.8cm, bottom=1.8cm, outer=1.8cm, inner=1.8cm, heightrounded, marginparwidth=2.5cm, marginparsep=0.5cm]{geometry}
\usepackage{enumitem}
\setlist{noitemsep,parsep=2pt}
\newcommand{\highlight}[1]{\textcolor{kuleuven}{#1}}
\usepackage{pythonhighlight}
\usepackage{cleveref}
\usepackage{graphicx}

\usepackage{tikz}
\usepackage{subcaption}
\usepackage{amsmath, amssymb}
\usepackage{listings}

\usepackage{multirow}
\usepackage{geometry}
\usepackage{tabularx}
\newcommand{\matr}[1]{\mathbf{#1}}
\geometry{margin=1in} % Adjust margins as needed

\usetikzlibrary{shapes,arrows,positioning}



\usepackage[authoryear,round,longnamesfirst]{natbib}



\def\retake{0}
\newcommand{\switch}[2]{\ifnum\retake=0{#1}\else{#2}\fi}


\newcommand{\nextyear}{\advance\year by 1 \the\year\advance\year by -1}
\newcommand{\thisyear}{\the\year}
\newcommand{\deadlineCode}{\switch{December 31, \thisyear{} at 18:00 CET}{August 14, \thisyear{} at 18:00 CET}}
\newcommand{\deadlineReport}{\deadlineCode}

\newcommand{\ReplaceMe}[1]{{\color{blue}#1}}
\newcommand{\RemoveMe}[1]{{\color{purple}#1}}

\setlength{\parskip}{5pt}

%opening
\title{Evolutionary Algorithms: Final report}
\author{Fabian Denoodt (r0698535)}

\begin{document}
\fontfamily{ppl}
\selectfont{}

\maketitle

%\section{\RemoveMe{Formal requirements}} \label{sec_this}
%
%\RemoveMe{The report is structured for fair and efficient grading of over 100 individual projects in the space of only a few days. Please respect the exact structure of this document. You are allowed to remove sections \ref{sec_this} and \ref{sec_other}. Brevity is the soul of wit: a good report will be \textbf{around $7$ pages} long. The hard limit is 10 pages. 
%
%
%\begin{quote}
%Think of this report as a \textbf{take-home exam}; it will be used at the exam for structuring the discussion and questions. Make an effort so that it can be visually scanned efficiently, e.g., by using boldface or colors to highlight key points, using lists, clearly defined paragraphs, figures, etc.
%
%You do not need to explain in this report \textbf{how} the techniques and concepts that are literally in the slides work. The goal of this report is \textbf{not} to illustrate that you can reproduce the slides. You need to convince me that you aptly used these (and other) techniques in this project. If I have doubts about your understanding of certain concepts in the course materials, I will test this hypothesis at the exam.
%\end{quote}
%
%It is recommended that you use this \LaTeX{} template, but you are allowed to reproduce it with the same structure in a WYSIWYG-editor. The purple text containing our evaluation criteria can be removed. You should replace the blue text with your discussion.
%
%This report should be uploaded to Toledo by \deadlineReport. It must be in the \textbf{Portable Document Format} (pdf) and must be named \texttt{r0123456\_final.pdf}, where r0123456 should be replaced with your student number.}

\section{Metadata}

\begin{itemize}
 \item \textbf{Group members during group phase:} Sisheng Liu and Melvin Schurmans
 \item \textbf{Time spent on group phase:} 12 hours
 \item \textbf{Time spent on final code:} 168 hours or more (3 * 7d * 8h).
 \item \textbf{Time spent on final report:} 14 hours
\end{itemize}

\section{Changes since \switch{the group phase}{your previous submission} (target: $0.25$ pages)}

%\ReplaceMe{List the main changes that you implemented since \switch{the group phase}{your previous submission of the project}. You do not need to explain the employed techniques in detail; for this, you should refer to the appropriate subsection of section 3 of the report.}
\begin{enumerate}
	\item \textbf{Genetic algorithm} from base project.
	\begin{enumerate}
	\item \textbf{Variation} \\
			More mutation and crossover procedures: introduce \textbf{scramble \& inversion mutation} (preserve swap mutation from group project), introduce \textbf{order crossover}, preserve (edge crossover from group project).
				
	\item \textbf{Diversity} \\
			Through \textbf{fitness sharing} and \textbf{island model} \textbf{local search}.

	\item \textbf{Local search} \\
			Two-opt, Insert node at random location.
	
	\item \textbf{Phase-based convergence} \\
		Split optimization in \textbf{exploration phase (1)} for first half of the run (eg 2.5 minutes) and then go over to \textbf{exploitative convergence phase (2)} for the remaining duration.
		
	\item No self-adaptation or multi-objective optimization.
	\end{enumerate}
	
	\item \textbf{Plackett-Luce Gradient Search (PL-GS) model}: For this part, the base evolutionary algorithm is discarded and an algorithm based on gradient descent in the discrete domain (proposed by \citep{ceberiojosu_model-based_2023, santucci_gradient_2020}) is used instead. This part includes the following efforts:
		\begin{enumerate}
			\item Reproduce the results from their paper to their benchmark. This includes writing entire source code, which was not publicly available.
			\item Apply the PL-GS model to the Traveling Salesman Problem (TSP).
			\item Explore alternative representations to represent Probability Mass Function (PMF) in PL-GS (based on Probabilistic Graphical Models).
		\end{enumerate}
	
\end{enumerate}

	\textcolor{red}{
		We have made a modification to the structure of the report. Section \ref{cha:final_design} remains the final design of the algorithm. Here we only discuss the genetic algorithm, which is based on the group project. A new section (\ref{cha:pl-gs}) is added where we discuss the PL-GS algorithm. The performance of the algorithm is underwhelming and, therefore, not part of the final algorithm used for the performance evaluation of this course.
	}


\section{Final design of the evolutionary algorithm (target: $3.5$ pages)} \label{cha:final_design}
%\RemoveMe{\textbf{Goal:} Based on this section, we will evaluate insofar as you are able to design and implement an advanced, effective evolutionary algorithm for solving a model problem.}


\subsection{The three main features}
	%\ReplaceMe{List the three main components of your evolutionary algorithm for this project. That is, what are its most distinctive characteristics, what components am I not allowed to change to a more basic version? Ideally, these are some of the more advanced features that you added since \switch{the group phase}{your previous submission}.}

	\begin{enumerate}
	\item Order-crossover (only edge crossover is too exploitative, having a single child based on mutual edges from two parents resulted in too quick convergence to sub-optimal. Instead, order-crossover, obtains 2 children per two parents, which ensures that population doesn't convergence to same path.)
	\item Significant performance improvement through \textbf{local search} and \textbf{fitness sharing}.
	\item Phase-based convergence \\
		- Encouraging diversity (due to multiple islands running in tandem and order-crossover creating larger populations) results in large computational costs, possibly hurting performance. Instead, we allow for an explorative scheme through multiple islands in the first phase (with migration and so on). In the second phase, islands merge to a single island, preserving only the candidate solutions that survive the elimination procedure. Additionally, we only run edge-crossover during this phase as it is a more greedy procedure, converging faster.
	\end{enumerate}
	
	


\subsection{The main loop}
	%\ReplaceMe{Make a picture of the ``flow'' in your evolutionary algorithm, similar to the example below. Include all the main components (mutation, recombination, selection, elimination, initialization, local search operators, diversity promotion mechanisms). There are no formal requirements on how to do this, as long as it is clear and you can efficiently explain your complete evolutionary algorithm using this picture at the exam. Contrary to the picture below, include the specific techniques, e.g., top-$\lambda$ elimination, $k$-tournament selection, where possible.}
	We solely provide a figure for the genetic algorithm. Since the PL-GS algorithm does not fit in the ``selection, variation, evaluation"-scheme, we believe a high-level view of the pseudo-code is more insightful.
	
	\begin{figure}[h]
		\centering
		\begin{subfigure}{0.45\textwidth}
			\includegraphics[width=\linewidth, trim={4cm 6cm 4cm 6cm}, clip]{phase1.pdf}
			\caption{Phase 1: Explorative with multiple islands and mostly order crossover.}
			\label{fig:croppedfile1}
		\end{subfigure} 
		\begin{subfigure}{0.45\textwidth}
			\includegraphics[width=\linewidth, trim={4cm 6cm 4cm 6cm}, clip]{phase2.pdf}
			\caption{Phase 2: Exploitative with single island and only edge crossover and scramble mutation.}
			\label{fig:croppedfile2}
		\end{subfigure}
		\caption{Overview of the Genetic algorithm.}
		\label{fig:comparison}
	\end{figure}

	

\subsection{Representation}\label{sec_rep}
	%\ReplaceMe{How do you represent the candidate solutions? What is your motivation to choose this one? What other options did you consider? How did you implement this specifically in Python (e.g., a list, set, numpy array, etc)?}
	
	In both algorithms (Genetic algorithm and PL-GS), a route is indirectly represented using the adjacency representation $\sigma = \left( \sigma(1), \sigma(2) \dots, \sigma(n) \right) \in \mathbb{S}^n$, where $\mathbb{S}^n$ represents the set of all permutations of length $n$.\\
	\textbf{Note:} Implementation-wise, the permutations will be represented as numpy arrays with length $n-1$. The length can be set to $n-1$ by implicitly inferring that all representations start from city 1, hence this number does not need to be stored in the numpy array. This decision was made to enable a more computationally efficient implementation of constructing the ``edge-table" (used in edge crossover and when computing the distance of a node to another in fitness sharing). %\textbf{WHY??}
	
	As discussed in the group report, we considered the cycle notation as representation. Although this is the exact representation required for the edge list in the crossover procedure, it posed challenges in ensuring that the path is a single cycle. We thus chose the first representation because we presumed that the performance cost of computing an edge table would be more manageable than having to deal with the overhead of invalid individuals.
	

\subsection{Initialization}
	\textbf{Population initialization:}
	In both algorithms, population initialization is carried out randomly. Notes depending on the algorithm:
		\begin{enumerate}
			\item \textbf{Genetic algorithm}: \\
			Individuals are defined as random permutations, that do not pass through the same city twice. \textbf{No local search is applied during initialization}. We assume local search would be helpful at the start to remove infinity edges and give a small push towards the right solution. However, for the larger routes (e.g. 750 or 1000) our local search operators are quite computationally expensive. Instead, by a trick to the weights of infinite edges (see the paragraph below), enabling us to compare distances between the number of infinity edges, we can get away with not applying local search in the first round. Being able to avoid the computational cost from local search at the start is nice for computational reasons, but also ensures that we do not need to think about whether local search would push all populations from each island into too narrow regions, resulting in faster convergence in the beginning but a sub-optimal solution in the end. Local search will be applied at every iteration afterwards. 
			\textbf{\item PL-GS}:\\
			Initialization of the population $\{ \sigma^{(1)} \dots \sigma^{(\lambda)} \} $ is dependent on the definition of $p_\mathbf{w}(\cdot)$, which by default is defined as a uniform categorical distribution. Implementation-wise this is done by defining $\mathbf{w}$ as a numpy array containing $n$ zeros. \\
			\textbf{Alternative representations:} Although not explored in this project, an island model could be considered in combination with the PL-GS algorithm, each with a different initialization for $p_\mathbf{w}$ than the uniform distribution. This allows for easily searching different regions of the solution space. The island model in combination with PL-GS was not explored because a migration procedure is not trivial to apply to probability density functions.
		\end{enumerate}	


		
	\textbf{Distance matrix initialization:} 
	\begin{enumerate}
	\item An infinite edge $D_{ij}$ is replaced by a real value according to the following equation:
	$$D_{ij} \leftarrow \text{largest non-inf edge} \times n $$
	As such, we can compare paths containing infinite edges based on how many infinite edges they have. By multiplying by $n$ in the equation, we ensure that choosing a path without infinite edges is always better than a path with infinite edges, regardless of how long the non-infinite path may be. This enables faster convergence in the early stages of the algorithm.
	\item Secondly, since in the PL-GS algorithm parameter updates are based on how large $f$ outcomes are, we normalize by dividing each edge distance by the largest non-infinite value. This also helps to prevent overflow errors caused by exploding gradients. Normalization is only applied in PL-GS.	
	\end{enumerate}

	%\ReplaceMe{How do you initialize the population? How did you determine the number of individuals? Did you implement advanced initialization mechanisms (local search operators, heuristic solutions)? If so, describe them. Do you believe your approach maintains sufficient diversity? How do you ensure that your population enrichment scheme does not immediately take over the population? Did you implement other initialization schemes that did not make it to the final version? Why did you discard them? How did you determine the population size?}



\subsection{Selection Operators}
	The selection process, k-tournament with replacement, remains unchanged from the group project. The value of the parameter $k$ is predetermined and remains constant for a specific TSP, such as 750 or 1000 tours. While it is possible to extend the selection operators by adjusting $k$ dynamically, for instance, starting with a low $k$ for more randomness and gradually increasing it towards the end for a higher probability that the good candidates are always selected, we opted not to pursue this approach. This decision is based on the fact that we did not assign the responsibility of managing population diversity or promoting exploitation to the selection process. These aspects are already taken care off by other components in our algorithm, such as fitness sharing for diversity, local search for exploitation, and the phase-based convergence scheme (that works with varying numbers of islands and crossover functions depending on the need for diversity or exploitation). Moreover, maintaining a constant selection scheme adds predictability to the algorithm, in contrast to introducing a parameter that changes smoothly over time.

		

	
	
	%\ReplaceMe{Which selection operators did you implement? If they are not from the slides, describe them. Can you motivate why you chose this one? Are there parameters that need to be chosen? Did you use an advanced scheme to vary these parameters throughout the iterations? Did you try other selection operators not included in the final version? Why did you discard them?}
	

\subsection{Mutation operators}
	\textbf{Evolutionary Algorithm:}
	We considered the following three mutation operators: \textbf{Inverse mutation, swap mutation and scramble mutation}. We observed that scramble mutation resulted in slightly better results compared to the other two algorithms. We therefore choose the following scheme: 
	\begin{enumerate}
		\item Phase 1) Three islands run in tandem, each with its own mutation operator. The mutation rate remains fixed and is identical for each island.
		\item Phase 2) At the beginning of phase 2, the islands merge and the best mutation operator is preserved (scramble mutation in this case).
	\end{enumerate}
	Although we observed better results using scramble mutation, we preserved the other mutation techniques in phase one, encouraging islands to explore different regions. The mutation rate percentage is a parameter that is decided upfront, dependent on the number of tours.
	

	%\ReplaceMe{Which mutation operators did you implement? If they are not from the slides, describe them. How do you choose among several mutation operators? Do you believe it will introduce sufficient randomness? Can that be controlled with parameters? Do you use self-adaptivity? Do you use any other advanced parameter control mechanisms (e.g., variable across iterations)? Did you try other mutation operators not included in the final version? Why did you discard them?}

\subsection{Recombination operators}
	In the group project, we used \textbf{edge crossover} because the technique chooses the mutual edges between two parents, such that the ``good properties" from a candidate would be preserved into the offspring. While being a good exploitative technique, from preliminary testing we observed that the entire population quickly converged to a single candidate solution, even after introducing other diversity mechanisms such as islands and fitness sharing. To combat this premature convergence, we introduced \textbf{order crossover} as well. Order crossover can create two offspring from two parents, while aiming to preserve the relative order from the parents. In our experiments, we observed that this less exploitative technique delays the entire population from converging to a single individual as quickly.
	
	Based on these findings, we apply the following scheme:
	\begin{enumerate}
		\item Phase 1) All three islands have 80\% of applying order crossover (for diversity) and 20\% of edge crossover (for exploitation). Order crossover creates twice the number of offspring than edge crossover, but the elimination procedure then reduces the offspring population to the required population size.
		\item Phase 2) The objective is now to obtain a single and optimal candidate. Hence in phase 2) edge crossover is constantly applied.
	\end{enumerate}


	%\ReplaceMe{Which recombination operators did you implement? If they are not from the slides, describe them. How do you choose among several recombination operators? Why did you choose these ones specifically? Explain how you believe that these operators can produce offspring that combine the best features from their parents. How does your operator behave if there is little overlap between the parents? Can your recombination be controlled with parameters; what behavior do they change? Do you use self-adaptivity? Do you use any other advanced parameter control mechanisms (e.g., variable across iterations)? Did you try other recombination operators not included in the final version? Why did you discard them? Did you consider recombination with arity strictly greater than 2?}

\subsection{Elimination operators}
	%\ReplaceMe{Which elimination operators did you implement? If they are not from the slides, describe them. Why did you select this one? Are there parameters that need to be chosen? Did you use an advanced scheme to vary these parameters throughout the iterations? Did you try other elimination operators not included in the final version? Why did you discard them?}
	
	The elimination operator remains k-tournament selection (without replacement), as in the group project. The parameter $k$ is set to the same value as in selection. Elimination also considers the parents.
	
	Although we did not explore it in this project, another interesting technique would be age-based elimination. This would be less computationally demanding, and the possible dangers of bad offspring are already reduced by local search. Using age-based elimination would allow us to compute local search only for the candidate solutions that are guaranteed to go through. In our current elimination operator (k-tournament), we risk creating offspring and applying expensive local search procedures to offspring that do not survive the following rounds.
	

\subsection{Local search operators} \label{cha:local search}
	We consider three operators:
	\begin{enumerate}
		\item \textbf{Two-opt:} our implementation is based on the pseudo-code found online\footnote{https://en.wikipedia.org/wiki/2-opt}. There are two implementations to consider, two-opt for symmetric matrices or for asymmetric matrices. Although in theory, the symmetric variant is not applicable to our TSP, we observed it still significantly improved performance, while being much easier to compute than in the asymmetric case. Hence, we use the efficient two-opt for symmetric distance matrices, choosing two nodes, swapping them, and reversing the path between the two nodes.
		
		\item \textbf{Insert node at random location:}\\
		\textbf{Motivation:} since our two-opt implementation is in theory not guaranteed to actually improve candidate solutions (due to symmetric distance matrix assumption), we also experiment with other local search procedures, that ensure a correct transformation, while remaining relatively easy to compute (in contrast to two-opt in the asymmetric case). \\
		\textbf{Explanation:} Given a permutation $\sigma$, this procedure considers a subset of random node pairs $\left(a = \sigma(i), b = \sigma(j)\right)$ with $i \neq j$. For each pair, the procedure verifies if inserting $b$ just in front $a$ results in a better fitness (this can be calculated in constant time, not linear). If performance is improved, the node is inserted, resulting in the original nodes $\sigma(i+1) \dots \sigma(j-1)$ being moved one step to the right. The subset size is a parameter determined upfront.

		\item \textbf{PL-GS:} \\
		\textbf{Motivation:} the algorithm (discussed in section \ref{cha:pl-gs}), although initially introduced as a standalone evolutionary algorithm by \citeauthor{santucci_gradient_2020}, could in theory also be used a as local search operator for a single $\sigma$. By default PL-GS results in finding a PMF $p_\mathbf{w}(\cdot)$ where sampling from it should give us a $\sigma$ for which $f(\sigma)$ is good. If we start the algorithm, not from the uniform categorical distribution $p_\mathbf{w}(\cdot)$, but instead, define $\mathbf{w}$ s.t. $p_\mathbf{w}(\cdot)$ is high for $\sigma$ and low for other routes, applying the PL-GS algorithm for a few steps, would result in similar samples to $\sigma$ which are hopefully better. \\
		Implementation-wise, given $\sigma = \left( \sigma(1), \sigma(2), \sigma(3),~\dots,~\sigma(n) \right)$, then $p_\mathbf{w}(\cdot)$ can be initialized by defining $\mathbf{w}_{\sigma(1)} = 1,~\mathbf{w}_{\sigma(2)} = \frac{1}{2},~\dots,~\mathbf{w}_{\sigma(n)} = \frac{1}{n}$, where $\mathbf{w}_i$ refers to the $i$'th component in the vector $\mathbf{w}$. Although the idea seems appealing in theory, our preliminary tests have shown that the PL-GS converges slowly, and a simple two-opt procedure obtains better results in a shorter duration. We therefore do not consider it as part of the final algorithm.
	\end{enumerate}
	A single local search operator and its respective tuning parameters are predetermined and dependent on the TSP.
		


%\ReplaceMe{What local search operators did you implement? Describe them. Did they cause a significant improvement in the performance of your algorithm? Why (not)? Did you consider other local search operators that did not make the cut? Why did you discard them? Are there parameters that need to be determined in your operator? Do you use an advanced scheme to determine them (e.g., adaptive or self-adaptive)?}

\subsection{Diversity promotion mechanisms} \label{cha:diversity}
	%\ReplaceMe{Did you implement a diversity promotion scheme? If yes, which one? If no, why not? Describe the mechanism you implemented. In what sense does the mechanism improve the performance of your evolutionary algorithm? Are there parameters that need to be determined? Did you use an advanced scheme to determine them?}
	
	\begin{enumerate}
		\item \textbf{Fitness sharing} \\
		The distance metric $d$ is defined as follows: 
		$$d(\sigma_1, \sigma_2) = n - \# \text{mutual edges}(\sigma_1, \sigma_2)$$
		Here, $n$ refers to the number of nodes. Hence, by constructing an edge table for $\sigma_1$, $~d(\sigma_1, \sigma_2)$ can be computed in linear time complexity.
		We fix $\sigma_{\text{share}} = n$, s.t. given a candidate solution $\sigma$, its shared fitness score is dependent on the distance to every other node in the population. Setting $\sigma_{\text{share}} < n$ resulted in unpredictable results, as sometimes $\sigma$ would be punished, and sometimes not. The parameter $\alpha$ is determined dependent on the number of tours (e.g.: 50 or 750).
		%\textbf{+ argument TODO } also scared that 2 good candidate (but similar) solutions would both be punished too hard, resulting in only bad but different candidates to survive selection.
		
		\textbf{Time complexity and an efficient approximation}: computing the average distance of a single permutation $\sigma$ to all other nodes would require rougly $\Theta(n^2)$ computations. Hence, if this metric must be computed for every individual in the population, the algorithm quickly becomes slow. Instead, we do two things:
		\begin{enumerate}
			\item The fitness scores are computed for only a sub-population (eg 10\%). The remaining 90\% then receive a ``fake" fitness score based on the average penalty of the sub-population.
			\item For each individual part of the 10\% sub-population, the distance metric is computed with a subset of nodes (for instance, 10\%). We ensure that each node is guaranteed to be compared against itself as well.
		\end{enumerate}
		Both parameters are tuned based on the TSP problem.


		
		\item \textbf{Multiple phases \& islands in tandem}
		The initial algorithm starts from \textbf{three islands}, each island with its own mutation operator to encourage. Migration between islands occurs every 25 epochs. During migration, for each island $i$ a predetermined percentage of its population (e.g. 10\%) is moved to the next island ($(i+1) \%3$).
		
		Running multiple islands in tandem is significantly slower than running one island. It may also not be beneficial to maintain all islands during the entire execution of the algorithm. When diversity is required, then the computational cost of multiple islands may be acceptable. However, after a certain duration the goal is no longer diversity, but instead, to obtain a good solution. Hence, one island may be better. We therefore introduce two phases:
		\begin{enumerate}
			\item Phase 1) 3 islands in tandem, with migration after 25 epochs.
			\item Phase 2) After a certain duration (e.g. 2.5 minutes), merge all islands into a single island (through the elimination procedure discussed above).
		\end{enumerate}
		
		The parameter when to go over to phase 2 is also determined in advance, dependent on the number of tours in the TSP.  We explored multi-threading, but this did not seem to improve performance.
		 	
	
	\end{enumerate}
		


\subsection{Stopping criterion}
For the five-minute benchmarks, no stopping criterion is applied. Even if an algorithm's population may have converged to a single individual, there may still be a slim chance of obtaining a better result due to mutation. 

For the histogram experiment with 500 runs, running the algorithm for 5 minutes each time, was not an option due to time constraints. Here we set a smaller time window that is roughly based on our approximate ``dissimilarity" metric. The metric is defined as follows:
$$
\frac{1}{P} \sum_{\substack{i~\in~\{1 \dots P \}}} d(\sigma_i, \sigma_{(i+1) \% P})
$$
Here, $P \in \mathbb{N}$ is the population size and $d()$ is the same distance function discussed in section \ref{cha:diversity}. The metric can be computed with a time complexity of roughly $\Theta(P \times n)$.
Intuitively, the metric compares each node to one other node and computes the average distance. Based on this metric we have an idea of how similar the population is, e.g.: 747.5 in a 750-tours TSP implies a lot of dissimilarity, while 0 implies that each node is the same.

Based on this population-similarity metric, we set the stopping time for the 50-tour TSP to 20 seconds per run. This results in approximately the final 50 iterations to be stuck at the identical population (50 iterations roughly corresponds to 5 seconds, with a single island, or 20\% of the running duration).

%\ReplaceMe{Which stopping criterion did you implement? Did you combine several criteria?}


\subsection{Parameter selection}
	A grid search, trying every combination is not possible due to the large number of parameters running each for 5 minutes. Instead, we start from a base set of parameters and optimize one parameter at a time. The order in which we considered the parameters and the respective values we experimented with are shown in table \ref{tbl:order-of-params}. This procedure is applied to all TSPs. Afterwards, we tweak the parameters by hand based on intuition. The final parameters per number of tours is shown in table \ref{tbl:params}.

	\begin{table}[ht]
		\centering
		\begin{tabularx}{0.9\textwidth}{|X|X|}
			\hline
			\textbf{Parameter} & \textbf{Values} \\
			\hline
			Population size & 10, 50, 100, 200, 500, 1000 \\
			\hline
			Offspring size multiplier & 1, 2, 3 \\
			\hline
			k (k-tournament in selection \& elimination) & 3, 5, 10, 25 \\
			\hline
			Mutation rate (\%) & 0.05, 0.2, 0.4 \\
			\hline
			Migrate every nb epochs & 25, 50 \\
			\hline
			Migration percentage & 0.05, 0.1 \\
			\hline
			Merge after percent time left & 0.5, 0.75, 0.9 \\
			\hline
			Fitness sharing subset percentage & 0.05, 0.2, 0.5 \\
			\hline
			Alpha (in fitness sharing) & 1, 2, 0.5 \\
			\hline
			Local search (either 2-opt or insert random node) & None, (2-opt, 1), (2-opt, 5), (insert, 0.1), (insert, 0.5), (insert, 1) \\
			\hline
		\end{tabularx}
		\caption{Parameters for TSP in order of which one was optimized first.}
		\label{tbl:order-of-params}
	\end{table}

	






\section{Plackett-Luce Gradient Search - An advanced EDA scheme} \label{cha:pl-gs}
	Estimation of Distribution (EDA) schemes replace standard variation operators, and instead, consider a model (e.g.: a graphical model) of which samples are taken, evaluated, and based on these scores the model is adjusted to hopefully create better samples in the future \cite{eiben_introduction_2015}. In this section, we explore such a scheme based on \citeauthor{santucci_gradient_2020}'s gradient search algorithm, ``Plackett-Luce Gradient Search (PL-GS)".
	
	\subsection{Concise explanation of the algorithm}
	The paper introduces a gradient-based optimization scheme, applied to a discrete domain. In particular, to the space of permutations $\sigma \in \mathbb{S}^n$. 
	Since $f : \mathbb{S}^n \rightarrow \mathbb{R}$ has a discrete domain, one cannot directly apply a gradient descent scheme to optimize $f$ as follows:
	$$
	\sigma \leftarrow \sigma + \eta \nabla f(\sigma)
	$$
	Indeed, $f$'s domain is discrete, and therefore, $\nabla f(\sigma)$ is not defined. Instead, the authors aim to optimize $f(\cdot)$ indirectly, based on the following objective function:
	$$F(\mathbf{w}) = E_\mathbf{w}[f(\sigma)] = \sum_{\sigma \in \mathbb{S}^n} f(\sigma) p_\mathbf{w}(\sigma)$$
	Here, $p_\mathbf{w}(\sigma)$ is a differentiable probability mass function with parameters $\mathbf{w} \in \mathbb{R}^n$. We discuss two possible representations in the following two sections. Note that finding the parameters $\mathbf{w}$ for which $F(\mathbf{w})$ is optimal, will also result in samples $\sigma \sim p_\mathbf{w}(\sigma)$ for which $f(\sigma)$ is good.
	
	One can prove that the following statements hold true \citep{santucci_gradient_2020}:
	%\nabla F_\mathbf{w}(\mathbf{w}) = E_\mathbf{w} \left[f(\sigma)\delta log\right]
	\begin{align}
	\nabla F_\mathbf{w}(\mathbf{w}) & = \sum_{\sigma \in \mathbb{S}^n} f(\sigma) \left[ \nabla_\mathbf{w} \log p_\mathbf{w}\right] p_\mathbf{w}(\sigma) = E_\mathbf{w} \left[f(\sigma) \nabla_\mathbf{w} \log p_\mathbf{w}(\sigma) \right] \\
									& \approx \frac{1}{\lambda} \sum_{i=1}^{\lambda} f(\sigma^{(i)}) \nabla \log p_\mathbf{w}(\sigma^{(i)})
	\end{align}
	Equation 1) tells us that the gradient of $F$ can be computed, regardless of whether $f$ is differentiable or not. We merely require that that $p_\mathbf{w}$ is differentiable with respect to $\mathbf{w}$. Equation 2) shows how to approximate $\nabla F_\mathbf{w}(\mathbf{w})$ based on $\lambda$ samples.
	
	
	


%
%	3 main features: \\
%	\begin{enumerate}
%		% \item \ReplaceMe{Just state the feature, you do not need to explain it, instead refer to the appropriate section below.}
%		\item Each candidate solution $\sigma$ is samples from a distribution $p_\mathbf{w}$ where the parameters $\mathbf{w}$ are optimized.
%		\item The parameters $\mathbf{w}$ are optimized through gradient gradient ascent. (Although the fitness function $f$ is not necessarily differentiable, though proper choice of a good representation for $p_\mathbf{w}$ will be w.r.t. $\mathbf{w}$)
%		\item $p_\mathbf{w}$ is represented as a categorical distribution based on the Plackett-Luce ranking model.
%	\end{enumerate}
	
	Based on the above details, we obtain the following algorithm to find $\sigma$ for which $f(\sigma)$ is good:\\
	\textbf{PL-GS:} \textit{high-level view of the pseudo-code:} \\
	\textbf{Objective:} Optimize $F(\mathbf{w}) = E_\mathbf{w}[f(\sigma)] = \sum_{\sigma \in \mathbb{S}^n} f(\sigma) p_\mathbf{w}(\sigma)$ \\	
	\textbf{Initialization:} Define the uniform categorical distribution $p_\mathbf{w}(\sigma)$ s.t. $\forall~\sigma: p_\mathbf{w}(\sigma)=\frac{1}{|\mathbb{S}^n|}$ \\	
	\textbf{Repeat until convergence:}
	\begin{enumerate}
		\item Compute $\lambda$ samples: $\sigma^{(i)} \sim p_\mathbf{w}(\sigma)$.
		\item Based on samples $\sigma^{(1)}, \dots, \sigma^{(\lambda)}$, calculate $\nabla_{\mathbf{w}} F\left(\mathbf{w}\right)$.
		\item Update $\mathbf{w} \leftarrow \mathbf{w} - \eta \nabla F\left(\mathbf{w}\right)$.
	\end{enumerate}
	\textbf{Return $\sigma \sim p_\mathbf{w}(\sigma)$}.
	
	\subsubsection{Plackket-Luce ranking model as PMF} \label{cha:pmf_indep}
	As a representation for $p_\mathbf{w}$, the authors consider an extension on the Plackett-Luce ranking model. Let $p_\mathbf{w}\left(\sigma(i) \mid D\right)$ denote the probability of traversing the $i$'th node $\sigma(i)$, given a set of nodes $D$ which we have not yet traversed, then $p_\mathbf{w}\left(\sigma(i) \mid D\right)$ is defined as follows:
	$$
		p_\mathbf{w}\left(i=\sigma(i) \mid D\right)=\frac{\mathbf{w}_i}{\sum_{\mathbf{w}_j \in D} \mathbf{w}_j}
	$$
	Here, $\mathbf{w} \in \mathbb{R}^n$ is a vector where the components $\mathbf{w}_i$ contain the non-normalized probabilities of sampling node $i$. Hence, using this model, one can model a PMF over the permutation space $\mathbb{S}^n$ as follows:	
	\begin{align}
		p_\mathbf{w}(\sigma) &= p_\mathbf{w}\left(\sigma(1) \mid \{ \sigma(1) \dots \sigma(n) \}\right) \cdot p_\mathbf{w}\left(\sigma(2) \mid \{ \sigma(2) \dots \sigma(n) \} \right) \cdot \ldots \cdot p_\mathbf{w}\left(\sigma(n) \mid \{ \sigma(n) \} \right) \\
		&=\prod_{i=1}^{n-1} \frac{\mathbf{w}_{\sigma(i)}}{\sum_{j=i}^n \mathbf{w}_{\sigma(j)}}
	\end{align}
	Intuitively, a sampling procedure for this PMF is obtained by sampling $n$ nodes from a categorical distribution with parameters $\mathbf{p} = (\mathbf{w}_1 \dots \mathbf{w}_n)$ without replacement. Such sampling procedure can be efficiently implemented using the ``Gumbel trick" \cite{kool_stochastic_2019}. Our Python implementation includes this trick. \\
	\textbf{Note:} the definition of $p_\mathbf{w}(\sigma)$ in the paper is more mathematically involved, including multiple $\exp()$ functions to ensure that $\mathbf{w}$ consists of positive values. We omit this notation in the equation here for simplicity.

	As shown in equation (1), the gradient of $\nabla F_\mathbf{w} \in \mathbb{R}^n$, requires the computation of $\nabla \log p_\mathbf{w}(\sigma)$. By applying Calculus rules, one can derive the following partial derivative \citep{santucci_gradient_2020}:
	$$
	\frac{\partial \log p_\mathbf{w}(\sigma)}{\partial \mathbf{w}_{\sigma(i)}}=1-\exp \mathbf{w}_{\sigma(i)} \sum_{k=1}^i \frac{1}{\sum_{j=k}^n \exp \mathbf{w}_{\sigma(j)}}
	$$
	Note that the paper contains an error. The equation shown here is correct.
	
	\subsubsection{An extension on the PMF}
	Here, we explore another representation for $p_\mathbf{w}$ which is not part of \cite{santucci_gradient_2020, ceberiojosu_model-based_2023}. 
	
	\textbf{Motivation:} A noticeable limitation of the $p_\mathbf{w}$ representation in \ref{cha:pmf_indep} is the independence assumption between nodes. E.g.: traversing (= sampling) node $a$ will not influence whether the next node to be traversed is node $b$ or $c$. However, this is an incorrect assumption in the TSP which could drastically hurt the performance. For instance, when the algorithm has learned the optimal path to be $a, b, c, \dots $. Now consider an unlucky node being sampled at the start, e.g. $b$ is sampled first. Using the representation from the $p_\mathbf{w}$, the next node is most likely to be $a$, however, is no longer the best next node the traverse, resulting in reduced scores. Instead, $c$ would have been the correct node.
	
	Another note is related to the number of learned paths. A single $p_\mathbf{w}$ learns a single optimal path, but perhaps it could be interesting to also consider other paths. We therefore represent $p_\mathbf{w}$ as a first order Markov chain network, where sampling a node $a$ at time step $t$ is dependent on the previously sampled node at time step $t-1$. Its Bayesian network is depicted in figure \ref{fig:bayesian-network}. Formally, $p_\matr{w}$ is defined as follows:
	$$
	\begin{aligned}
		p_\matr{W}(\sigma) &= p\left(\sigma(1)\right) \cdot \prod_{t=2}^{n-1} p\left(\sigma(t) \mid \sigma(t-1) \right) \\
						   &= \frac{1}{n} \prod_{t=2}^{n-1} \frac{\matr{W}_{\sigma(t) \mid \sigma(t-1)}}{\sum_{j=t}^n \matr{W}_{\sigma(j) \mid \sigma(t-1)}} \\
	\end{aligned}
	$$
	Here, $\matr{W}$ is an $n \times n$ matrix where $\matr{W}_{ij} = \matr{W}_{i | j}$ denotes the probability of sampling node $i$ given that $j$ has just been sampled. We assume the probability of traversing the first-node as uniform, hence the $\frac{1}{n}$ in the second equation.
	
	\begin{figure}
		\centering
		\includegraphics[width=0.7\linewidth, trim={8cm 10cm 8cm 10cm}, clip]{"bayesian network"}
		\caption{Bayesian network: our representation for $p_\mathbf{w}$}
		\label{fig:bayesian-network}
	\end{figure}
	
	The gradient $\nabla F_\mathbf{\matr{W}} \in \mathbb{R}^{n\times n}$ is now a square matrix, based on $\nabla \log p_\matr{W}(\sigma)$. The partial derivative is defined similar as before for $t \in \{ 2 \dots n \}$:
	$$
	\frac{\partial \log p_\mathbf{w}(\sigma)}{\partial \matr{w}_{\sigma(t) \mid \sigma(t-1)}} = 1-\exp \matr{w}_{\sigma(t) \mid \sigma(t - 1)} \sum_{k=2}^t \frac{1}{\sum_{j=k}^n \exp \matr{w}_{\sigma(j) \mid \sigma(t - 1)}}
	$$
	In addition,
	$$
	\frac{\partial \log p_\mathbf{w}(\sigma)}{\partial \matr{w}_{i \mid j}} = 0
	$$
	when $i$ does not succeed $j$.
	
	\subsection{Considerations for future work}
	\begin{enumerate}
		\item
	The algorithm as introduced by \citeauthor{santucci_gradient_2020} does not contain any \textbf{diversity population }mechanisms. However, approaches similar to fitness sharing could be adapted to this algorithm as well. A penalty could be added to the fitness $f(\cdot)$ in the form of a regularization term, based on the average distance of $\lambda$ samples. We could express the resulting function $g(\cdot)$ as follows:
	$$
	g(\sigma^{(i)}) = f(\sigma^{(i)}) + \frac{1}{\lambda} \sum_{\substack{\sigma^{(j)}, \sigma^{(k)} \\ \in \{ \sigma^{(1)} \dots \sigma^{(\lambda)} \}}} d(\sigma^{(j)}, \sigma^{(k)})
	$$

	\item Although selection is not directly applicable to PL-GS since there is no population to apply variation operators on, one could pose the question which samples are allowed to influence the gradient $\nabla F\left(\mathbf{w}\right)$, used to update $\mathbf{w}$:
	$$\mathbf{w} \leftarrow \mathbf{w} - \eta \nabla F\left(\mathbf{w}\right)$$
	By default all $\{ \sigma^{(1)} \dots \sigma^{(\lambda)} \} $ are used with equal weighting. However, also here, \textbf{selection} procedures such as k-tournament could be applied to, for instance, only select the good samples to update $\mathbf{w}$, while eliminating the bad ones and thus preventing them from updating $\mathbf{w}$ in a bad direction.
	\end{enumerate}
		
%	\subsubsection{Limitations?} \textbf{TODO}
%	%The model enables us to assign a probability value to each, given a set of nodes to consider.
%	
%	limitations:
%	- scalability,
%	- numerical stability
%	- failed representation, encouraging small and large values, inherently unstable.	
%	- my implemention: no such thing as the gumbell trick, so sampling is much slower. also not every parameter updated from a single sample, this also results in slower performance.
	

\section{Numerical experiments (target: 1.5 pages)}
	%\RemoveMe{\textbf{Goal:} Based on this section and our execution of your code, we will evaluate the performance (time, quality of solutions) of your implementation and your ability to interpret and explain the results on benchmark problems.}

\subsection{Metadata} \label{cha:meta}
	All parameters are presented in table \ref{tbl:params}. Overall, the parameters remain relatively constant depending on the number of tours. There are two exceptions; local search and $k$ in k-tournament. Although two-opt obtained good results for smaller tours, it does not scale to larger tours. We did experiment with applying two-opt to only a subset of nodes in a path, but although less computationally demanding, it did not always result in good candidate solutions. This may be a side effect of the symmetric matrix assumption we've made to improve time complexity. Two-opt reverses the path between two nodes, and as the number of tours increases, the length of these reversed lists also increases. This may result in the error due to the symmetric distance matrix assumption to also increase, resulting in a higher probability of faulty solutions. As a compensation for the lack of a good local search procedure in the larger tours, we increase $k$ to increase convergence speed, as the good individuals in the populations have a larger chance of surviving.


	%\ReplaceMe{What parameters are there to choose in your evolutionary algorithm? Which fixed parameter values did you use for all experiments below? If some parameters are determined based on information from the problem instance (e.g., number of cities), also report their specific values for the problems below.


	%Report the main characteristics of the computer system on which you ran your evolutionary algorithm. Include the processor or CPU (including the number of cores and clock speed), the amount of main memory, and the version of Python 3.}
	PC characteristics:
	\begin{enumerate}
		\item Processor Intel i7-4790 CPU \@ 3.60GHz (Cores: 4)
		\item Memory 16GB
		\item Python version 3.9
	\end{enumerate}
	


	

\subsection{tour50.csv}	
	Figures \ref{fig:meanfitnesses} and \ref{fig:tour50csvmeanobjectivevalue} display the performance graphs. The algorithm is run for exactly 20 seconds. The first 10\% in the convergence graph (fig \ref{fig:tour50csvmeanobjectivevalue}) have been removed. The graph depicts the results from a single island. Note that the duration in the graph is not entirely linear. The first 25 iterations take 10 seconds, while the remaining 100 iterations also take 10 seconds. This is due to the first iterations being more computationally demanding due to having three islands in tandem.
	
	To gain some insight in a single run, we refer to table \ref{tbl:convergence}. Due to the two-opt local search, the algorithm finds a better solution than the greedy heuristic provided on Toledo within 25 iterations. Meanwhile, due to the order crossover and other diversity mechanisms, the population can remain diverse in phase one (the maximum distance is 50 for 50-tours). Phase two behaves similarly to the egg-holder algorithm where the entire population quickly converges to a single solution. This is due to the edge crossover operator used in phase 2. We observe that the average distance between individuals quickly converges to zero, indicating that the entire population has become the same individual.
	
	The histogram (figure \ref{fig:meanfitnesses}) indicates quite consistent results. The algorithm consistently finds candidate solutions which are better than the greedy heuristic from Toledo.
	An example candidate solution with score of 25975.70 is the following: (0 26 21	32	17	40	34	33	44	36	23	28	27	12	25	39	37	20	49	22	6	9	43	30	48	5	8	46	35	16	4	15	47	42	11	3	19	38	10	13	7	41	14	1	18	24	45	2	31	29).


	%\ReplaceMe{Run your algorithm on this benchmark problem (with the 5 minute time limit from the Reporter). \textbf{Include a typical convergence graph, by plotting the mean and best objective values in function of the time} (for example based on the output of the Reporter class).
	%	
	%What is the best tour length you found? What is the corresponding sequence of cities?
	%	
	%Interpret your results. How do you rate the performance of your algorithm (time, memory, speed of convergence, diversity of population, quality of the best solution, etc)? Is your solution close to the optimal one?
	%	
	%\textbf{Solve this problem 500 times and record the results. Make a histogram of the final mean fitnessess and the final best fitnesses of the 500 runs. Comment on this figure: is there a lot of variability in the results, what are the means and the standard deviations?}
	%}
	
	
	\begin{table}[ht]
		\centering
		\small
		\begin{tabular}{|l|c|c|c|c|}
			\hline
			\textbf{Parameter} & \textbf{50 Tours} & \textbf{100 Tours} & \textbf{500 Tours} & \textbf{1000 Tours} \\
			\hline
			\textbf{Popul Size} & 50 & 50 & 50 & 50 \\
			\textbf{Offspring size mult.} & 1 & 1 & 1 & 1 \\
			\textbf{K (k-tournament)} & 3 & 3 & 10 & 25 \\
			\textbf{Mutation rate} & 0.1 & 0.2 & 0.2 & 0.2 \\
			\textbf{Migrate after epochs} & 25 & 25 & 25 & 5 \\
			\textbf{Migration \%} & 0.05 & 0.05 & 0.05 & 0.05 \\
			\textbf{Merge islands after \% time left} & 0.5 & 0.5 & 0.5 & 0.5 \\
			\textbf{Fitness sharing sub-population \%} & 0.05 & 0.05 & 0.05 & 0.05 \\
			\textbf{Local search} & 2-opt & 2-opt & IRN, 0.5 & IRN, 0.5 \\
			\hline
		\end{tabular}
		\caption{Parameter settings for different tours. IRN refers to the Insert Random Node operator and 0.5 is the number of nodes to consider, as discussed in section \ref{cha:local search}}
		\label{tbl:params}
	\end{table}
	
	
		
	\begin{table}[ht]
		\centering
		\begin{tabular}{|c|c|c|c|c|c|c|c|}
			\hline
			\textbf{Phase} & \textbf{Island} & \textbf{Iteration} & \textbf{Best Fitness} & \textbf{Avg Fitness} & \textbf{Fit Shared} & \textbf{Avg Dist} & \textbf{Mutation} \\
			\hline
			\multirow{3}{*}{1} & 0 & 24 & 27025.15 & 146528.29 & 59479.35 & 46.86 & Inversion\\
			& 1 & 24 & 26792.80 & 126835.99 & 66089.59 & 46.57 & Swap \\
			& 2 & 24 & 26736.30 & 124162.91 & 122564.17 & 46.47 & Scramble\\
			\hline
			\multirow{4}{*}{2} & NA & 49 & 25957.36 & 26759.92 & 1066930.26 & 0.69 & Scramble\\
			& NA & 74 & 25957.36 & 26020.75 & 981034.99 & 1.02 & Scramble \\
			& NA & 99 & 25981.84 & 25981.82 & 1065255.44 & 0.00 & Scramble\\
			& NA & 124 & 25981.84 & 25981.84 & 1065255.44 & 0.00 & Scramble\\
			\hline
		\end{tabular}
		\caption{Example run on the 50 tours TSP, 20 seconds. Both phases run for 10 seconds each.}
		\label{tbl:convergence}
	\end{table}
	
	




\subsection{tour100.csv}\label{sec_shorttour}
Figure \ref{fig:tour100} shows a typical convergence graph of the algorithm running for five minutes. Similar as discussed before, due to islands running in tandem in phase 1), the first 125 roughly take 2.5 minutes and the remaining 375 iterations, the other 2.5 minutes. Here, the difference between the two phases is clearly noticeable, having phase one with a more exploration objective, while phase aims to exploit, resulting in a behavior similar to the egg-holder algorithm. We observe that the algorithm quite easily outperforms the simple greedy heuristic from Toledo. We observe that similar to the 50-tours problem, the algorithm does not require the entire 5 minutes to find a good solution.

%\ReplaceMe{Run your algorithm on this benchmark problem (with the 5 minute time limit from the Reporter). \textbf{Include a typical convergence graph, by plotting the mean and best objective values in function of the time} (for example based on the output of the Reporter class).
%
%What is the best tour length you found in each case? 
%
%Interpret your results. How do you rate the performance of your algorithm (time, memory, speed of convergence, diversity of population, quality of the best solution, etc)? Is your solution close to the optimal one?}

\subsection{tour500.csv and tour1000.csv}
Figures \ref{fig:tour500} and \ref{fig:tour1000} depict the convergence graphs over 5 minutes. With the larger maps, we observe that problems start to arise, as we can no longer beat the greedy heuristic from Toledo within the 5 minute interval. This behavior can be partly explained due to the local search operator. As discussed in section \ref{cha:meta}, our two-opt local search operator is computationally demanding, and does not scale well to the larger tours. We therefore, need to use a less demanding local search operator (insert random node), which clearly has impact. As a consequence of the slow convergence, we end up having to increase the selection pressure to $k=10$  for 500 tours and $k=25$ for 1000 tours. This results in slightly faster due to less randomness, but such large $k$ values are unusual as the population size is only 50. Most likely, in the long run, this procedure, in combination with the egg-holder effect in phase 2 will almost guarantee we will eventually get stuck in local optimum. However, we also experimented with less selection pressure, but this would converge slower, resulting in a worse overall sore at the end of the 5 minute mark.

\subsection{PL-GS}
Finally, we discuss the two PL-GS algorithms for the 50 tours TSP. The results are shown in figures \ref{fig:placketluceindendent} (PMF with independence assumption) and \ref{fig:placketluceconditional} (PMF with conditional assumption (ours)). Both algorithms ran for one minute. While both algorithms seem nice in practice, the graphs demonstrate that both algorithms are immediately outperformed by even a simple evolutionary algorithm (eg without diversity promotion or local search). The independent PMF obtained a final best fitness of $76,999.85$ and population average fitness of $158,960,90$, the conditional PMF a final best fitness of $83,706.50$ and population average fitness of $1,316,909.39$ indicating that both algorithms do converge but slowly.

While techniques exist to speed up convergence, e.g.: using a larger learning rate, the PL-GS algorithm is inherently unstable due to the representation of both PMFs. Hence, choosing to increasing the learning may increase the chances of overflow errors. This is due to the design of the Plackett-Luce model, assigning a probability score of sampling a node at time step $t$. A converged model, which consistently results in similar samples thus requires the probability weight of sampling the first node to be large (towards infinity), while later nodes should have a lower probability, with the final approaching 0. As the length of the permutation becomes longer, this problem becomes even more prominent. Hence, scalability is a major issue for this model.

We also observe that our proposed PMF representation, converges slower than the independent PMF. This can be explained due to the number of parameters which must be optimized, which is now $n^2$, instead of $n$. A second reason, is that a sample $\sigma \sim p_\matr{w}(\sigma)$ can only influence the parameters $\matr{W}_{t|t-1}$ for $t \in \{2 \dots n\}$. The remaining weights can only be updated if they are part of another permutation sample.

%Although in theory, the latter PMF representation should be able to learn more complex distributions, we observed it got stuck in local minima faster than the independent PMF.

	


\section{Critical reflection (target: 0.75 pages)}

%\ReplaceMe{What are the three main strengths of evolutionary algorithms in your experience?}
Three positive points:
\begin{enumerate}
 \item Having a phase-based scheme with a tweakable a parameter on when to go to phase two allows for an easily adaptive algorithm depending on the problem. For easy problems, with few local minima, phase 2 can start early allowing to find optimum quickly. For complicated problems, the first phase can go on for a long time without unwillingly converging to a population of identical individuals. Secondly, since only phase 1) contains multiple islands, we gain the population diversity benefits,  while the computational drawbacks can be tempered as phase 2) merges all islands and preserves the best ones.

\item Our two-opt local search operator which assumes a symmetric matrix can still perform well for smaller TSP problems, resulting in the algorithm quickly converging to a good solution. The symmetric matrix assumption allows for a much faster implementation than if the asymmetry must be considered.

\item Our fitness sharing implementation encourages diversity, while not being a big burden on speed due to a fast approximation of the shared fitness values of the population.
\end{enumerate}

Three weak points:
\begin{enumerate}
\item Since our best local search operator (two-opt) did not scale to larger problems we have a less powerful operator for the larger tours. This can be observed in the performance graphs of the larger tours, as the algorithm converges much slower. As a consequence, we aim to increase convergence speed in other ways due to an unusually large $k$ value, which may increase the chances of getting stuck in a local minimum.
\item While the objective of the second phase is to quickly converge to the optimal solution, given the series of good candidates found in phase 1), we found that phase 2) may behave too much like the egg-holder algorithm, quickly eliminating any diversity. This is largely due to the edge crossover operator, only preserving mutual edges between individuals.

\item The island model, although resulting in improved performance, remains computationally demanding. Multi-threading could help preserve the benefits while omitting the computational costs.

\item While we do have parameters depending on the number of tours, the parameters remain relatively static during the entire run (apart from the two phases). It would be interesting to investigate more dynamic parameters that change throughout the run. For instance, a dynamic $k$ value in k-tournament, requesting more diversity (low $k$) at the start and more selection pressure towards the end. Alternatively, a fitness-sharing scheme that only "kicks in" when the population diversity becomes too low, could also be interesting to explore. These dynamic models come at the cost of less predictability and this is the main reason that we did not explore them in this project.
\end{enumerate}


I initially started this assignment with the mindset of avoiding evolutionary algorithms at all costs, as they are known to be slow. Therefore, I implemented the gradient descent scheme, PL-GS. However, it quickly became apparent that such a scheme on its own would never be able to find an absolute optimum, as it kept getting stuck in local optima. In contrast, even the simplest evolutionary algorithm could outperform a greedy metric, although, with perhaps a slightly longer running time.

This experience has truly shown how evolutionary algorithms benefit from utilizing diversity schemes to avoid getting stuck in local optima. Additionally, they can employ local search schemes to find solutions relatively quickly. We can easily choose a balance in the trade-off between speed (exploitation) and finding the optimal answer (exploration). This is a benefit not immediately applicable to solely gradient descent schemes, for instance.





\begin{figure}[htbp]
	\centering
	\begin{minipage}{0.5\linewidth}
		\centering
		\includegraphics[width=0.9\linewidth]{mean_fitnesses}
		\caption{50 tours: 500 runs, averages/standard \\ deviations are displayed in the top right corner.}
		\label{fig:meanfitnesses}
	\end{minipage}%
	\begin{minipage}{0.5\linewidth}
		\centering
		\includegraphics[width=0.9\linewidth]{tour_50_csv_mean_objective_value}
		\caption{50 tours: 20 seconds, first 10\% have been skipped.}
		\label{fig:tour50csvmeanobjectivevalue}
	\end{minipage}%
	
	\begin{minipage}{0.5\linewidth}
		\centering
		\includegraphics[width=0.9\linewidth]{tour_100}
		\caption{100 tours}
		\label{fig:tour100}
	\end{minipage}%
	\begin{minipage}{0.5\linewidth}
		\centering
		\includegraphics[width=0.9\linewidth]{tour_500}
		\caption{500 tours}
		\label{fig:tour500}
	\end{minipage}
	
	\begin{minipage}{0.5\linewidth}
		\centering
		\includegraphics[width=0.9\linewidth]{tour_1000}
		\caption{1000 tours}
		\label{fig:tour1000}
	\end{minipage}%
\end{figure}

\begin{figure}[htbp]
	\centering
	\begin{subfigure}{0.48\linewidth}
		\includegraphics[width=\linewidth]{vanilla2.pdf}
		\caption{PL-GS (Independence assumption)}
		\label{fig:placketluceindendent}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.48\linewidth}
		\includegraphics[width=\linewidth]{cond2.pdf}
		\caption{PL-GS (Conditional assumption)}
		\label{fig:placketluceconditional}
	\end{subfigure}
	\caption{PL-GS Figures}
\end{figure}

\bibliographystyle{apalike}
\bibliography{references}



\end{document}
