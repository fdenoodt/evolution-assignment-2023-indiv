
@article{ceberiojosu_model-based_2023,
	title = {Model-based {Gradient} {Search} for {Permutation} {Problems}},
	copyright = {Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.},
	url = {https://dl.acm.org/doi/10.1145/3628605},
	doi = {10.1145/3628605},
	abstract = {Global random search algorithms are characterized by using probability distributions
to optimize problems. Among them, generative methods iteratively update the distributions
by using the observations sampled. For instance, this is the case of the well-...},
	language = {EN},
	urldate = {2023-12-29},
	journal = {ACM Transactions on Evolutionary Learning and Optimization},
	author = {CeberioJosu and SantucciValentino},
	month = dec,
	year = {2023},
	note = {Publisher: ACM
New York, NY},
	file = {Snapshot:C\:\\Users\\fabia\\Zotero\\storage\\EPDMTN4C\\3628605.html:text/html},
}

@misc{kool_stochastic_2019,
	title = {Stochastic {Beams} and {Where} to {Find} {Them}: {The} {Gumbel}-{Top}-k {Trick} for {Sampling} {Sequences} {Without} {Replacement}},
	shorttitle = {Stochastic {Beams} and {Where} to {Find} {Them}},
	url = {http://arxiv.org/abs/1903.06059},
	doi = {10.48550/arXiv.1903.06059},
	abstract = {The well-known Gumbel-Max trick for sampling from a categorical distribution can be extended to sample \$k\$ elements without replacement. We show how to implicitly apply this 'Gumbel-Top-\$k\$' trick on a factorized distribution over sequences, allowing to draw exact samples without replacement using a Stochastic Beam Search. Even for exponentially large domains, the number of model evaluations grows only linear in \$k\$ and the maximum sampled sequence length. The algorithm creates a theoretical connection between sampling and (deterministic) beam search and can be used as a principled intermediate alternative. In a translation task, the proposed method compares favourably against alternatives to obtain diverse yet good quality translations. We show that sequences sampled without replacement can be used to construct low-variance estimators for expected sentence-level BLEU score and model entropy.},
	urldate = {2023-12-29},
	publisher = {arXiv},
	author = {Kool, Wouter and van Hoof, Herke and Welling, Max},
	month = may,
	year = {2019},
	note = {arXiv:1903.06059 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: ICML 2019 ; 13 pages, 4 figures},
	file = {arXiv Fulltext PDF:C\:\\Users\\fabia\\Zotero\\storage\\4RZT4PVS\\Kool et al. - 2019 - Stochastic Beams and Where to Find Them The Gumbe.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\fabia\\Zotero\\storage\\S7T5KI32\\1903.html:text/html},
}

@inproceedings{santucci_gradient_2020,
	address = {New York, NY, USA},
	series = {{GECCO} '20},
	title = {Gradient search in the space of permutations: an application for the linear ordering problem},
	isbn = {978-1-4503-7127-8},
	shorttitle = {Gradient search in the space of permutations},
	url = {https://dl.acm.org/doi/10.1145/3377929.3398094},
	doi = {10.1145/3377929.3398094},
	abstract = {Gradient search is a classical technique for optimizing differentiable functions that has gained much relevance recently due to its application on Neural Network training. Despite its popularity, the application of gradient search has been limited to the continuous optimization and its usage in the combinatorial case is confined to a few works, all which tackle the binary search space. In this paper, we present a new approach for applying Gradient Search to the space of permutations. The idea consists of optimizing the expected objective value of a random variable defined over permutations. Such a random variable is distributed according to the Plackett-Luce model, and a gradient search over its continuous parameters is performed. Conducted experiments on a benchmark of the linear ordering problem confirm that the Gradient Search performs better than its counterpart Estimation of Distribution Algorithm: the Plackett-Luce EDA. Moreover, results reveal that the scalability of the Gradient Search is better than that of the PL-EDA.},
	urldate = {2023-12-29},
	booktitle = {Proceedings of the 2020 {Genetic} and {Evolutionary} {Computation} {Conference} {Companion}},
	publisher = {Association for Computing Machinery},
	author = {Santucci, Valentino and Ceberio, Josu and Baioletti, Marco},
	month = jul,
	year = {2020},
	keywords = {gradient search, optimization, permutations, symmetric group},
	pages = {1704--1711},
	file = {Full Text PDF:C\:\\Users\\fabia\\Zotero\\storage\\9JLCBQLM\\Santucci et al. - 2020 - Gradient search in the space of permutations an a.pdf:application/pdf},
}

@book{eiben_introduction_2015,
	address = {Berlin, Heidelberg},
	series = {Natural {Computing} {Series}},
	title = {Introduction to {Evolutionary} {Computing}},
	isbn = {978-3-662-44873-1 978-3-662-44874-8},
	url = {https://link.springer.com/10.1007/978-3-662-44874-8},
	language = {en},
	urldate = {2023-12-31},
	publisher = {Springer},
	author = {Eiben, A.E. and Smith, J.E.},
	year = {2015},
	doi = {10.1007/978-3-662-44874-8},
	keywords = {Estimation of Distribution Algorithms (EDA), Evolution Strategies (ES), Evolutionary Algorithm (EA), Evolutionary Computing (EC), Evolutionary Programming (EP), Evolutionary Robotics, Genetic Algorithms (GA), Genetic Programming (GP), Learning Classifier Systems (LCS), Memetic Algorithms, Optimization},
	file = {Submitted Version:C\:\\Users\\fabia\\Zotero\\storage\\FDYRB6QF\\Eiben and Smith - 2015 - Introduction to Evolutionary Computing.pdf:application/pdf},
}
